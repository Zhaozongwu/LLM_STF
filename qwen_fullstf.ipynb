{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rcm2XFsGG2-",
        "outputId": "7b886796-5181-4aaa-b0c0-e8e6f275b38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Qwen3-8B'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 53 (delta 15), reused 50 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (53/53), 1.73 MiB | 247.00 KiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "\n",
            "Exiting because of \"interrupt\" signal.\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!git clone https://www.modelscope.cn/Qwen/Qwen3-8B.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "11gDgjQWGR2V",
        "outputId": "f376054b-c870-47eb-e092-7afabdb6cbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.10.7-py3-none-any.whl.metadata (59 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/59.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.10.8 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.10.8-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Collecting datasets!=4.0.*,!=4.1.0,>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.10.1)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
            "Collecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3 (from unsloth)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.23.0,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.20.0)\n",
            "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth)\n",
            "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (0.22.1)\n",
            "Collecting torchao!=0.14.0,>=0.13.0 (from unsloth_zoo>=2025.10.8->unsloth)\n",
            "  Downloading torchao-0.13.0-1-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.10.8->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.10.8->unsloth) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.10.8->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting mistral_common (from unsloth_zoo>=2025.10.8->unsloth)\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.13.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral_common->unsloth_zoo>=2025.10.8->unsloth) (2.11.10)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common->unsloth_zoo>=2025.10.8->unsloth) (4.25.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common->unsloth_zoo>=2025.10.8->unsloth) (0.12.0)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common->unsloth_zoo>=2025.10.8->unsloth)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.22.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common->unsloth_zoo>=2025.10.8->unsloth) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common->unsloth_zoo>=2025.10.8->unsloth) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common->unsloth_zoo>=2025.10.8->unsloth) (0.27.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common->unsloth_zoo>=2025.10.8->unsloth) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common->unsloth_zoo>=2025.10.8->unsloth) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common->unsloth_zoo>=2025.10.8->unsloth) (0.4.2)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common->unsloth_zoo>=2025.10.8->unsloth)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.3.1)\n",
            "Downloading unsloth-2025.10.7-py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m346.9/346.9 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.3/506.3 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.10.8-py3-none-any.whl (269 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m269.2/269.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading torchao-0.13.0-1-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchao, shtab, pycountry, pyarrow, msgspec, tyro, pydantic-extra-types, xformers, transformers, datasets, cut_cross_entropy, bitsandbytes, trl, mistral_common, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: torchao\n",
            "    Found existing installation: torchao 0.10.0\n",
            "    Uninstalling torchao-0.10.0:\n",
            "      Successfully uninstalled torchao-0.10.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.48.1 cut_cross_entropy-25.1.1 datasets-4.2.0 mistral_common-1.8.5 msgspec-0.19.0 pyarrow-21.0.0 pycountry-24.6.1 pydantic-extra-types-2.10.6 shtab-1.7.2 torchao-0.13.0 transformers-4.56.2 trl-0.23.0 tyro-0.9.35 unsloth-2025.10.7 unsloth_zoo-2025.10.8 xformers-0.0.32.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install swanlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K2pBV47PGTdD",
        "outputId": "6ef97655-473b-48b9-8cb0-35e10c4ccedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swanlab\n",
            "  Downloading swanlab-0.6.12-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3>=1.35.49 (from swanlab)\n",
            "  Downloading boto3-1.40.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore (from swanlab)\n",
            "  Downloading botocore-1.40.56-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from swanlab) (8.3.0)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from swanlab) (12.575.51)\n",
            "Requirement already satisfied: platformdirs>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (5.9.5)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (2.11.10)\n",
            "Collecting pyecharts>=2.0.0 (from swanlab)\n",
            "  Downloading pyecharts-2.0.9-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from swanlab) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (2.32.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (13.9.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from swanlab) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from swanlab) (1.17.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.35.49->swanlab)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.35.49->swanlab)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore->swanlab) (2.9.0.post0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->swanlab) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->swanlab) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->swanlab) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->swanlab) (0.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from pyecharts>=2.0.0->swanlab) (3.1.6)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.12/dist-packages (from pyecharts>=2.0.0->swanlab) (3.16.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from pyecharts>=2.0.0->swanlab) (3.20.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->swanlab) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->swanlab) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.0->swanlab) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.6.0->swanlab) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.6.0->swanlab) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->swanlab) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->pyecharts>=2.0.0->swanlab) (3.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable->pyecharts>=2.0.0->swanlab) (0.2.14)\n",
            "Downloading swanlab-0.6.12-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m305.8/305.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.56-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.56-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m151.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyecharts-2.0.9-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.9/153.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, pyecharts, botocore, s3transfer, boto3, swanlab\n",
            "Successfully installed boto3-1.40.56 botocore-1.40.56 jmespath-1.0.1 pyecharts-2.0.9 s3transfer-0.14.0 swanlab-0.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!swanlab login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa3gDjgYGVRi",
        "outputId": "ba68bb94-1ee5-4584-aaa4-32fe5cd1d746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m You can find your API key at: \u001b[33mhttps://swanlab.cn/space/~/settings\u001b[0m\n",
            "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Paste an API key from your profile and hit enter, or press \u001b[32m'CTRL + C'\u001b[0m \n",
            "to quit: \n",
            "\u001b[2K\u001b[32mâ ‡\u001b[0m Waiting for the swanlab cloud response.\n",
            "\u001b[1A\u001b[2K\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m\u001b[1;34m \u001b[0mLogin successfully. Hi, \u001b[1manthoy\u001b[0m!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import unsloth\n",
        "import os\n",
        "import swanlab\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from unsloth import UnslothTrainer,FastQwen3Model,UnslothTrainingArguments\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import get_peft_model,LoraConfig\n",
        "#trust_remote_code=True æ„æ€æ˜¯æ˜¯å¦è¦å¯ç”¨tokenization_qwen.pyã€configuration_qwen.pyç­‰æ–‡ä»¶\n",
        "model,tokenizer = FastLanguageModel.from_pretrained(\"Qwen3-8B\",\n",
        "                         trust_remote_code=True,\n",
        "                         load_in_4bit=False,\n",
        "                         max_seq_length=1024,\n",
        "                         dtype=None,\n",
        "                         device_map=\"auto\")\n",
        "os.makedirs('./qwen8b_unsloth_with_lora', exist_ok=True)\n",
        "os.makedirs('./logs', exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229,
          "referenced_widgets": [
            "b114b4d8195b46a6a163ddaf1d7f26bd",
            "24c80668262e447287f1ce73bda5a1cb",
            "3934aab374a9427698ef75b7b626f849",
            "cec130b467344f018730240047a0dffb",
            "e725bbc7f1384aab8ed5ac7d12247c1e",
            "72eac7115d514c0da728340ec136b9e5",
            "74cd18b4e37148148c9c04a1dde7e551",
            "faf552f2c8174d7d9ece19aadb45ecf4",
            "c95a843a3ad04182a72767aca859168b",
            "b023d1aae21747e19d852e4f482ce281",
            "5712241734a64fcaba1a4aea6decbec9"
          ]
        },
        "id": "scY-637JGWlI",
        "outputId": "e0353205-a6ce-4675-9e69-c3256c175d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.10.7: Fast Qwen3 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b114b4d8195b46a6a163ddaf1d7f26bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# Load the training and evaluation datasets\n",
        "train_data = load_dataset('json', data_files='dataset.json',split='train[:80%]') #splitå‚æ•°æŒ‡å®šè¦æ¶å­å•Šæ•°æ®é›†åˆ’åˆ†\n",
        "test_data = load_dataset('json', data_files='dataset.json',split='train[80%:]')\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eLp8_3_GY73",
        "outputId": "b6da5166-a0f3-4282-bf48-9b91ee932380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output'],\n",
              "    num_rows: 1192\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_formatting_func(examples):\n",
        "  conversations = []\n",
        "  instruction = examples[\"instruction\"]\n",
        "  input_text = examples[\"input\"]\n",
        "  output = examples[\"output\"]\n",
        "  for instr, inp, out in zip(instruction, input_text, output):\n",
        "      # Concatenate prompt + label\n",
        "      conversations.append([{\"role\":\"user\",\"content\":instr + inp},\n",
        "                 {\"role\":\"assistant\", \"content\":out}])\n",
        "  return {\"text\": conversations}\n",
        "\n",
        "train_texts = train_data.map(make_formatting_func, batched = True)[\"text\"]\n",
        "train_dataset = [tokenizer.apply_chat_template(conv, tokenize=False) for conv in train_texts]\n",
        "\n",
        "test_texts = test_data.map(make_formatting_func, batched = True)[\"text\"]\n",
        "test_dataset = [tokenizer.apply_chat_template(conv, tokenize=False) for conv in test_texts]\n",
        "\n",
        "train_dataset[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604,
          "referenced_widgets": [
            "b10800d040b146b1a85adab25b8d66d4",
            "911094bd96a2477ca60457628e3821f0",
            "3f67b4bba34b4d4095c5c742e25f412a",
            "3e5758534c734430be6a78e025a6e5dd",
            "8996b2d25a7e40289309592befd5e1fd",
            "d8e62c06b85f47c9b4beb3ad69e971e4",
            "0b0d7b3e13ab47d18f4c42bc5ac81dbc",
            "dd27d7a916f04e8983de9e4c6852480a",
            "6cfddc68a35f48879efffb342767c1fc",
            "1796200378e54eefab0c286574217b0a",
            "847e624e32414abd8e941535fc3e5c3e",
            "7aaf5acbb7034f35985834de7c6c0488",
            "37e0a6f587a3418bade413d7d60c9b2a",
            "2ac8e8d1db4c4216b6e654e6cda184fa",
            "909d0215268e4758b13a22c542aea1f9",
            "24c72f8973274493af68ba9306098517",
            "6f0b392d94e24d95bc981ffb3250624e",
            "f6771c090b354b31bdbd48b9d09730ef",
            "cef698adf71e490fba52c956d68d2060",
            "f0965f6dede74e7ea71dcca5a13a32fe",
            "af173c9dd7434d37ac47f8732ac93bff",
            "09858b13bbfc4cd5b3e1de72e4b0fe3b"
          ]
        },
        "id": "Y_tHkJ5EGhe-",
        "outputId": "e6aa953b-03ca-45ab-a5c3-1646883c697b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1192 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b10800d040b146b1a85adab25b8d66d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/298 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7aaf5acbb7034f35985834de7c6c0488"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|im_start|>user\\nä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè¯·ä½ å¸®æˆ‘æŠ½å–å‡ºå…³ç³»å†…å®¹ä¸º\"æ€§èƒ½æ•…éšœ\", \"éƒ¨ä»¶æ•…éšœ\", \"ç»„æˆ\"å’Œ \"æ£€æµ‹å·¥å…·\"çš„ç›¸å…³ä¸‰å…ƒç»„ï¼Œä¸‰å…ƒç»„å†…éƒ¨ç”¨\"_\"è¿æ¥ï¼Œä¸‰å…ƒç»„ä¹‹é—´ç”¨\\\\nåˆ†å‰²ã€‚æ–‡æœ¬ï¼šæ•…éšœç°è±¡ï¼šå‘åŠ¨æœºæ°´æ¸©é«˜ï¼Œé£æ‰‡å§‹ç»ˆæ˜¯ä½é€Ÿè½¬åŠ¨ï¼Œé«˜é€Ÿæ¡£ä¸å·¥ä½œï¼Œå¼€ç©ºè°ƒå°¤å…¶å¦‚æ­¤ã€‚<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nå‘åŠ¨æœº_éƒ¨ä»¶æ•…éšœ_æ°´æ¸©é«˜\\né£æ‰‡_éƒ¨ä»¶æ•…éšœ_ä½é€Ÿè½¬åŠ¨<|im_end|>\\n',\n",
              " '<|im_start|>user\\nä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè¯·ä½ å¸®æˆ‘æŠ½å–å‡ºå…³ç³»å†…å®¹ä¸º\"æ€§èƒ½æ•…éšœ\", \"éƒ¨ä»¶æ•…éšœ\", \"ç»„æˆ\"å’Œ \"æ£€æµ‹å·¥å…·\"çš„ç›¸å…³ä¸‰å…ƒç»„ï¼Œä¸‰å…ƒç»„å†…éƒ¨ç”¨\"_\"è¿æ¥ï¼Œä¸‰å…ƒç»„ä¹‹é—´ç”¨\\\\nåˆ†å‰²ã€‚æ–‡æœ¬ï¼š957å·æ±½è½¦æ•…éšœæŠ¥å‘Šæ•…éšœç°è±¡ä¸€è¾†2007å¹´é•¿ä¸°çŒè±¹é£è…¾6400è¶Šé‡è½¦ï¼Œè¡Œé©¶é‡Œç¨‹27000kmã€‚ç»´ä¿®å®¤å†…ç¯ä¸äº®ï¼Œæ›´æ¢å®¤å†…ç¯æ³¡åï¼Œå‘ç°å››ä¸ªè½¬å‘ç¯å¸¸äº®ï¼Œåé›¨åˆ®å’Œåå–·æ°´å¸¸å·¥ä½œï¼Œå¼€å…³ä¸èµ·ä½œç”¨ã€‚<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nå¼€å…³_éƒ¨ä»¶æ•…éšœ_ä¸èµ·ä½œç”¨\\nç»´ä¿®å®¤å†…ç¯_éƒ¨ä»¶æ•…éšœ_ä¸äº®<|im_end|>\\n',\n",
              " '<|im_start|>user\\nä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè¯·ä½ å¸®æˆ‘æŠ½å–å‡ºå…³ç³»å†…å®¹ä¸º\"æ€§èƒ½æ•…éšœ\", \"éƒ¨ä»¶æ•…éšœ\", \"ç»„æˆ\"å’Œ \"æ£€æµ‹å·¥å…·\"çš„ç›¸å…³ä¸‰å…ƒç»„ï¼Œä¸‰å…ƒç»„å†…éƒ¨ç”¨\"_\"è¿æ¥ï¼Œä¸‰å…ƒç»„ä¹‹é—´ç”¨\\\\nåˆ†å‰²ã€‚æ–‡æœ¬ï¼šæ•…éšœåŸå› æ²¹è´¨é—®é¢˜è¯Šæ–­æ’é™¤1ã€å·¥ä½œä»‹è´¨â‘ ç²˜åº¦è¦æ±‚ï¼šæ²¹æ¶²çš„å·¥ä½œç²˜åº¦èŒƒå›´è¦æ±‚ä¸º13~54cstã€‚ç²˜åº¦å¤ªå¤§ï¼Œæ˜“å¯¼è‡´å¸æ²¹å’Œæ’æ²¹ä¸ç•…ï¼Œä½¿è½¬å‘æ³µäº§ç”Ÿå™ªå£°ï¼›ç²˜åº¦å¤ªå°ï¼Œæ˜“å¯¼è‡´å¯†å°æ€§å’Œæ¶¦æ»‘æ€§ä¸è‰¯ï¼Œä½¿è½¬å‘æ³µæ•ˆç‡é™ä½æˆ–æ—©æœŸç£¨æŸã€‚â‘¡æ¸…æ´åº¦è¦æ±‚ï¼šæ²¹æ¶²åº”å¹²å‡€ï¼Œæ²¹æ¶²ä¸­ä¸åº”å«æœ‰æ°´ã€æ²‰æ·€ç‰©ã€é‡‘å±å±‘ã€æ£‰çº±å¤´ç­‰æ±¡ç‰©é¢—ç²’ï¼Œè¿‡æ»¤ç²¾åº¦ä¸º25Î¼mã€‚æ–°è½¦è¡Œä½¿2500å…¬é‡Œéœ€æ›´æ¢ä¸€æ¬¡æ¶²å‹æ²¹ï¼Œå¯¹æ²¹ç½ã€æ»¤æ²¹è£…ç½®å’Œç®¡è·¯è¿›è¡Œä»”ç»†æ¸…æ´—ï¼Œä»¥åæ¯è¡Œé©¶2ä¸‡å…¬é‡Œæ¢æ²¹ä¸€æ¬¡ï¼Œå¹¶æ¸…æ´—ï¼ˆæˆ–æŒ‰æ±½è½¦åˆ¶é€ å‚è§„å®šè¿›è¡Œï¼‰ã€‚æ‚è´¨ä¼šä½¿è½¬å‘æ³µå†…é˜»å°¼å­”å µå¡æˆ–ä½¿è°ƒå‹é˜€é˜€å£å½¢æˆé—´éš™ä»è€Œå¯¼è‡´å·¥ä½œå‹åŠ›ä¸æ­£å¸¸ï¼›æ‚è´¨å¯å¯¼è‡´é˜€èŠ¯å¡æ­»ï¼Œä»¥è‡´å‹åŠ›å’Œæµé‡ä¸æ­£å¸¸è¾“å‡ºï¼›æ‚è´¨å¯å¯¼è‡´è½¬å‘æ³µå†…æ‘©æ“¦ä½¿ç£¨æŸé€Ÿåº¦åŠ å¿«ã€‚â‘¢æ¨èä½¿ç”¨30~40å·ä½æ¸©æŠ—ç£¨æ¶²å‹æ²¹ï¼›10å·èˆªç©ºæ¶²å‹æ²¹ï¼›8å·æ¶²åŠ›ä¼ åŠ¨æ²¹ã€‚<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nè½¬å‘æ³µ_éƒ¨ä»¶æ•…éšœ_æ•ˆç‡é™ä½\\næ²¹æ¶²_éƒ¨ä»¶æ•…éšœ_ç²˜åº¦å¤ªå¤§\\né˜€èŠ¯_éƒ¨ä»¶æ•…éšœ_å¡æ­»\\nè½¬å‘æ³µ_éƒ¨ä»¶æ•…éšœ_æ‘©æ“¦\\nå‹åŠ›_æ€§èƒ½æ•…éšœ_ä¸æ­£å¸¸è¾“å‡º\\nè½¬å‘æ³µ_éƒ¨ä»¶æ•…éšœ_ç£¨æŸ\\næ²¹æ¶²_éƒ¨ä»¶æ•…éšœ_ç²˜åº¦å¤ªå°\\nè°ƒå‹é˜€é˜€å£_éƒ¨ä»¶æ•…éšœ_å·¥ä½œå‹åŠ›ä¸æ­£å¸¸\\næ²¹_éƒ¨ä»¶æ•…éšœ_è´¨é—®é¢˜\\nè½¬å‘æ³µ_éƒ¨ä»¶æ•…éšœ_å™ªå£°\\nè°ƒå‹é˜€é˜€å£_éƒ¨ä»¶æ•…éšœ_é—´éš™\\næµé‡_æ€§èƒ½æ•…éšœ_ä¸æ­£å¸¸è¾“å‡º\\nè½¬å‘æ³µå†…é˜»å°¼å­”_éƒ¨ä»¶æ•…éšœ_å µå¡<|im_end|>\\n',\n",
              " '<|im_start|>user\\nä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè¯·ä½ å¸®æˆ‘æŠ½å–å‡ºå…³ç³»å†…å®¹ä¸º\"æ€§èƒ½æ•…éšœ\", \"éƒ¨ä»¶æ•…éšœ\", \"ç»„æˆ\"å’Œ \"æ£€æµ‹å·¥å…·\"çš„ç›¸å…³ä¸‰å…ƒç»„ï¼Œä¸‰å…ƒç»„å†…éƒ¨ç”¨\"_\"è¿æ¥ï¼Œä¸‰å…ƒç»„ä¹‹é—´ç”¨\\\\nåˆ†å‰²ã€‚æ–‡æœ¬ï¼šå¤„ç†æ–¹æ³•(1)ç»•ç»„å—æ½®å¼•èµ·æ¥åœ°çš„åº”å…ˆè¿›è¡Œçƒ˜å¹²ï¼Œå½“å†·å´åˆ°60â€”â€”70â„ƒå·¦å³æ—¶ï¼Œæµ‡ä¸Šç»ç¼˜æ¼†åå†çƒ˜å¹²ã€‚(2)ç»•ç»„ç«¯éƒ¨ç»ç¼˜æŸåæ—¶ï¼Œåœ¨æ¥åœ°å¤„é‡æ–°è¿›è¡Œç»ç¼˜å¤„ç†ï¼Œæ¶‚æ¼†ï¼Œå†çƒ˜å¹²ã€‚(3)ç»•ç»„æ¥åœ°ç‚¹åœ¨æ§½å†…æ—¶ï¼Œåº”é‡ç»•ç»•ç»„æˆ–æ›´æ¢éƒ¨åˆ†ç»•ç»„å…ƒä»¶ã€‚æœ€ååº”ç”¨ä¸åŒçš„å…†æ¬§è¡¨è¿›è¡Œæµ‹é‡ï¼Œæ»¡è¶³æŠ€æœ¯è¦æ±‚å³å¯ã€‚<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nç»•ç»„_éƒ¨ä»¶æ•…éšœ_å—æ½®\\nç»ç¼˜_éƒ¨ä»¶æ•…éšœ_æŸå<|im_end|>\\n',\n",
              " '<|im_start|>user\\nä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè¯·ä½ å¸®æˆ‘æŠ½å–å‡ºå…³ç³»å†…å®¹ä¸º\"æ€§èƒ½æ•…éšœ\", \"éƒ¨ä»¶æ•…éšœ\", \"ç»„æˆ\"å’Œ \"æ£€æµ‹å·¥å…·\"çš„ç›¸å…³ä¸‰å…ƒç»„ï¼Œä¸‰å…ƒç»„å†…éƒ¨ç”¨\"_\"è¿æ¥ï¼Œä¸‰å…ƒç»„ä¹‹é—´ç”¨\\\\nåˆ†å‰²ã€‚æ–‡æœ¬ï¼šå¤„ç†åŸåˆ™ï¼šç«‹å³æ±‡æŠ¥å€¼ç­è°ƒæ§äººå‘˜ã€‚è‹¥ç›´æµç³»ç»ŸåŠŸç‡å›é™ï¼Œåˆ™æ£€æŸ¥ç›´æµç³»ç»Ÿè¿è¡Œæƒ…å†µï¼Œç«‹å³æ‰‹åŠ¨æŠ•å…¥å¤‡ç”¨äº¤æµæ»¤æ³¢å™¨ï¼›è‹¥å¤‡ç”¨äº¤æµæ»¤æ³¢å™¨æŠ•å…¥åç›´æµåŠŸç‡ä»ä¸èƒ½æ¢å¤åˆ°æ•…éšœå‰æ°´å¹³ï¼Œå¯æ ¹æ®æŠ•å…¥åè¾¾åˆ°çš„åŠŸç‡æ°´å¹³å‘å€¼ç­è°ƒæ§äººå‘˜ç”³è¯·è°ƒæ•´ç›´æµåŠŸç‡ã€‚è‹¥ç›´æµç³»ç»ŸåŒæé—­é”ï¼Œåˆ™æ£€æŸ¥ç«™å†…äº¤æµæ»¤æ³¢å™¨æ˜¯å¦å…¨éƒ¨é€€å‡ºï¼Œè‹¥æœªå…¨éƒ¨é€€å‡ºï¼Œåˆ™ç«‹å³æ‰‹åŠ¨é€€å‡ºã€‚åšå¥½å®‰æªï¼ŒåŠæ—¶å¤„ç†ã€‚<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nç›´æµç³»ç»Ÿ_éƒ¨ä»¶æ•…éšœ_åŠŸç‡å›é™\\nç›´æµç³»ç»ŸåŒæ_éƒ¨ä»¶æ•…éšœ_é—­é”\\nç›´æµåŠŸç‡_æ€§èƒ½æ•…éšœ_ä¸èƒ½æ¢å¤åˆ°æ•…éšœå‰æ°´å¹³<|im_end|>\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset_flat = [{\"text\":t} for t in train_dataset]\n",
        "test_dataset_flat = [{\"text\":t} for t in test_dataset]\n",
        "\n",
        "train_Data = Dataset.from_list(train_dataset_flat)\n",
        "test_Data = Dataset.from_list(test_dataset_flat)"
      ],
      "metadata": {
        "id": "U7k4ZLhPGirB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_training(model)\n",
        "swanlab.init(\"qwen8b_experiment_with_lora\")\n",
        "\n",
        "model = model.train()\n",
        "from trl import SFTTrainer,SFTConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(model,\n",
        "             r=8,\n",
        "             lora_alpha=16,\n",
        "             target_modules=['q_proj','v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "             lora_dropout=0.05,\n",
        "             bias='none',\n",
        "             use_gradient_checkpointing=\"unsloth\"\n",
        "                              )\n",
        "#æ‰“å°å½“å‰æ¨¡å‹ä¸­å“ªäº›å‚æ•°æ˜¯å¯è®­ç»ƒçš„ï¼Œæ•°é‡æ˜¯å¤šå°‘\n",
        "# ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°\n",
        "# for p in model.parameters():\n",
        "#     p.requires_grad = True\n",
        "\n",
        "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} \"\n",
        "#       f\"({trainable_params / total_params * 100:.2f}%)\")\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "trainer_args = SFTConfig(\n",
        "    output_dir='./qwen8b_unsloth_with_lora',\n",
        "    num_train_epochs=4,\n",
        "    dataset_text_field='text',\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_ratio=0.05,\n",
        "    learning_rate=1e-5,\n",
        "    fp16=False,\n",
        "    logging_steps=10,  #æ¯éš”10æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒloss\n",
        "    save_steps=50,\n",
        "    eval_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    report_to=['swanlab'],\n",
        "    run_name='qwen8b_experiment_with_lora'\n",
        "\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_Data,\n",
        "    eval_dataset=test_Data,\n",
        "    dataset_text_field='text',\n",
        "    args = trainer_args,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "56fc10ee119845c889bf411eb0a3cc5e",
            "5b4ec9c89ff740d2ae4c0e0b5b418c5b",
            "6f4814c9510d4bd587613c9e441dd374",
            "374b5d1fc4334336ab8f07c4ce7ea28c",
            "b4743475f4ca4a0494bc380651d19043",
            "996dc83e4c22450cbe265fe6d617f1e4",
            "3f38f302a77e4f54acf309ca0b023da1",
            "4dbe9a506dae450f9ccd2449c884337f",
            "0336d512ef244678b6cb2a9f5c5740a3",
            "728ed419523e42dc831c27abf453ba36",
            "599023835f9c4a3c963835c6506c47f5",
            "addd911cc9634fc99ed50cf0d7a6f192",
            "36b4d16ed3f84a54892789873fc7ba02",
            "354f122ee4a8442898bf64dc5bfd0fba",
            "8fa39dfba15f4e2db0f393c303ce0e28",
            "38da1368ac384d9bb63d2449f368593c",
            "4c5d889068774d598dd96eedbde82bbf",
            "209545bfa55648adac3efc8dc5f8923a",
            "73e77e0bc05643c893ce89097acc08eb",
            "45282a37eb074e578e3574127dcd80d2",
            "58392375d74e4c2087fd22b3a870f1d1",
            "20ac05e419e84785b8f64f01213d64b7"
          ]
        },
        "id": "rXuSTFgLGlmn",
        "outputId": "4e5f3a03-5496-402e-e3ad-d7d3708e2915"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> You have already initialized a run, the init function will be ignored\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;33mswanlab\u001b[0m\u001b[1;39m:\u001b[0m You have already initialized a run, the init function will be ignored\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 21,823,488 || all params: 8,212,558,848 || trainable%: 0.2657\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56fc10ee119845c889bf411eb0a3cc5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/1192 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "addd911cc9634fc99ed50cf0d7a6f192",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,192 | Num Epochs = 4 | Total steps = 596\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 21,823,488 of 8,212,558,848 (0.27% trained)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [455/596 40:47 < 12:41, 0.19 it/s, Epoch 3.05/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.322300</td>\n",
              "      <td>3.416855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.389800</td>\n",
              "      <td>3.397353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.239300</td>\n",
              "      <td>3.321832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.334700</td>\n",
              "      <td>3.130571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.991000</td>\n",
              "      <td>2.918282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.832900</td>\n",
              "      <td>2.766301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.618400</td>\n",
              "      <td>2.625415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.481400</td>\n",
              "      <td>2.483484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.263700</td>\n",
              "      <td>2.348993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.205100</td>\n",
              "      <td>2.227218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.118300</td>\n",
              "      <td>2.112748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.026400</td>\n",
              "      <td>2.004918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.833200</td>\n",
              "      <td>1.889579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.812200</td>\n",
              "      <td>1.778573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.666300</td>\n",
              "      <td>1.700279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.668700</td>\n",
              "      <td>1.653952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.523100</td>\n",
              "      <td>1.631135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.598700</td>\n",
              "      <td>1.611719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.643400</td>\n",
              "      <td>1.590386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.482200</td>\n",
              "      <td>1.581393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.428300</td>\n",
              "      <td>1.574378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.494400</td>\n",
              "      <td>1.567992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.537100</td>\n",
              "      <td>1.561579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.384800</td>\n",
              "      <td>1.555911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.508900</td>\n",
              "      <td>1.549669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.493000</td>\n",
              "      <td>1.543912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.472600</td>\n",
              "      <td>1.538628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.367900</td>\n",
              "      <td>1.533445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.426600</td>\n",
              "      <td>1.528826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.405500</td>\n",
              "      <td>1.524993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.429900</td>\n",
              "      <td>1.521255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.437700</td>\n",
              "      <td>1.516911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.372600</td>\n",
              "      <td>1.513686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.389200</td>\n",
              "      <td>1.510309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.466700</td>\n",
              "      <td>1.507354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.444900</td>\n",
              "      <td>1.504774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.428300</td>\n",
              "      <td>1.502412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.472700</td>\n",
              "      <td>1.500394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.378200</td>\n",
              "      <td>1.498299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.505000</td>\n",
              "      <td>1.496678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.439100</td>\n",
              "      <td>1.494996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.333300</td>\n",
              "      <td>1.493829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.410000</td>\n",
              "      <td>1.492157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.477900</td>\n",
              "      <td>1.490863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.418600</td>\n",
              "      <td>1.488856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='596' max='596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [596/596 53:32, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.322300</td>\n",
              "      <td>3.416855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.389800</td>\n",
              "      <td>3.397353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.239300</td>\n",
              "      <td>3.321832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.334700</td>\n",
              "      <td>3.130571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.991000</td>\n",
              "      <td>2.918282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.832900</td>\n",
              "      <td>2.766301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.618400</td>\n",
              "      <td>2.625415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.481400</td>\n",
              "      <td>2.483484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.263700</td>\n",
              "      <td>2.348993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.205100</td>\n",
              "      <td>2.227218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.118300</td>\n",
              "      <td>2.112748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.026400</td>\n",
              "      <td>2.004918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.833200</td>\n",
              "      <td>1.889579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.812200</td>\n",
              "      <td>1.778573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.666300</td>\n",
              "      <td>1.700279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.668700</td>\n",
              "      <td>1.653952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.523100</td>\n",
              "      <td>1.631135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.598700</td>\n",
              "      <td>1.611719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.643400</td>\n",
              "      <td>1.590386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.482200</td>\n",
              "      <td>1.581393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.428300</td>\n",
              "      <td>1.574378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.494400</td>\n",
              "      <td>1.567992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.537100</td>\n",
              "      <td>1.561579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.384800</td>\n",
              "      <td>1.555911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.508900</td>\n",
              "      <td>1.549669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.493000</td>\n",
              "      <td>1.543912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.472600</td>\n",
              "      <td>1.538628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.367900</td>\n",
              "      <td>1.533445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.426600</td>\n",
              "      <td>1.528826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.405500</td>\n",
              "      <td>1.524993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.429900</td>\n",
              "      <td>1.521255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.437700</td>\n",
              "      <td>1.516911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.372600</td>\n",
              "      <td>1.513686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.389200</td>\n",
              "      <td>1.510309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.466700</td>\n",
              "      <td>1.507354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.444900</td>\n",
              "      <td>1.504774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.428300</td>\n",
              "      <td>1.502412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.472700</td>\n",
              "      <td>1.500394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.378200</td>\n",
              "      <td>1.498299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.505000</td>\n",
              "      <td>1.496678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.439100</td>\n",
              "      <td>1.494996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.333300</td>\n",
              "      <td>1.493829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.410000</td>\n",
              "      <td>1.492157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.477900</td>\n",
              "      <td>1.490863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.418600</td>\n",
              "      <td>1.488856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.381200</td>\n",
              "      <td>1.487958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.474700</td>\n",
              "      <td>1.486943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.363900</td>\n",
              "      <td>1.485831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.369900</td>\n",
              "      <td>1.484859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.496800</td>\n",
              "      <td>1.484312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.443900</td>\n",
              "      <td>1.483858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.441900</td>\n",
              "      <td>1.482914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>1.381700</td>\n",
              "      <td>1.482294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.386400</td>\n",
              "      <td>1.481708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.493100</td>\n",
              "      <td>1.481286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.355700</td>\n",
              "      <td>1.480952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>1.355500</td>\n",
              "      <td>1.480705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.302500</td>\n",
              "      <td>1.480334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.445200</td>\n",
              "      <td>1.480412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "model,tokenizer = FastLanguageModel.from_pretrained(\"Qwen3-0.6B\",\n",
        "                         trust_remote_code=True,\n",
        "                         load_in_4bit=False,\n",
        "                         max_seq_length=1024,\n",
        "                         dtype=None,\n",
        "                         device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(model, './qwen_lora_unsloth_with_allitem/checkpoint-700')\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "model.save_pretrained('./qwen_lora_unsloth_allitem_merged')\n",
        "tokenizer.save_pretrained('./qwen_lora_unsloth_allitem_merged')\n",
        "print('Loraæƒé‡å·²åˆå¹¶å¹¶ä¿å­˜åˆ°æ–‡ä»¶./qwen_lora_unsloth_allitem_mergedä¸­')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "SDNpUreCGocb",
        "outputId": "54b0bc80-3e60-43f6-8f7b-9ce5aaebcf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.10.6: Fast Qwen3 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can't find 'adapter_config.json' at './qwen_lora_unsloth_with_allitem/checkpoint-700'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './qwen_lora_unsloth_with_allitem/checkpoint-700'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1165800340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                          device_map=\"auto\")\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./qwen_lora_unsloth_with_allitem/checkpoint-700'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_and_unload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 440\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 )\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at './qwen_lora_unsloth_with_allitem/checkpoint-700'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#è¿™æ˜¯å®æ—¶æµè¾“å‡ºçš„è¿‡ç¨‹\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# 1. åŠ è½½æ¨¡å‹\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"./qwen_unsloth_with_allitem\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16\n",
        ")\n",
        "\n",
        "prompt = (\n",
        "    'ä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè¯·ä½ å¸®æˆ‘æŠ½å–å‡ºå…³ç³»å†…å®¹ä¸º\"æ€§èƒ½æ•…éšœ\", '\n",
        "    '\"éƒ¨ä»¶æ•…éšœ\", \"ç»„æˆ\"å’Œ \"æ£€æµ‹å·¥å…·\"çš„ç›¸å…³ä¸‰å…ƒç»„ï¼Œä¸‰å…ƒç»„å†…éƒ¨ç”¨\"_\"è¿æ¥ï¼Œ'\n",
        "    'ä¸‰å…ƒç»„ä¹‹é—´ç”¨\\\\nåˆ†å‰²ã€‚æ–‡æœ¬ï¼š637å·æ±½è½¦æ•…éšœæŠ¥å‘Šæ•…éšœæè¿°å€’è½¦æˆ–è¸©åˆ¹è½¦æ—¶å’¯å™”ä¸€ä¸‹å¼‚å“'\n",
        "    'æ•…éšœåŸå› åˆ¹è½¦ç‰‡ä¸åˆ¹è½¦å¡é’³é—´éš™è¿‡å¤§é€ æˆçš„'\n",
        ")\n",
        "\n",
        "# 2. æ„å»ºèŠå¤©è¾“å…¥\n",
        "text = tokenizer.apply_chat_template(\n",
        "    [{\"role\":\"user\",\"content\":prompt}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking = False  #è¿™ä¸ªå†³å®šæ˜¯å¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹\n",
        "    )\n",
        "_= model.generate(\n",
        "    **tokenizer(text, return_tensors='pt').to(\"cuda\"),\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=False, #å…³é—­éšæœºé‡‡æ ·\n",
        "    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuzans0EGsmR",
        "outputId": "56e9aece-5820-43b5-e101-e4c77670b5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.10.6: Fast Qwen3 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "éƒ¨ä»¶æ•…éšœ_åˆ¹è½¦ç‰‡_å¼‚å“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "qwenæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ"
      ],
      "metadata": {
        "id": "NNZK5DmuGJnd"
      }
    }
  ]
}